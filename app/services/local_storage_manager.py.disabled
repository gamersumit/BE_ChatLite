"""
Local Storage Manager for Development Environment
Free alternative to cloud storage using local filesystem and vector DB.
"""

import os
import json
import hashlib
import sqlite3
import aiofiles
import gzip
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import logging

# Optional imports for vector capabilities
try:
    import chromadb
    from sentence_transformers import SentenceTransformer
    import numpy as np
    VECTOR_SUPPORT = True
except ImportError:
    VECTOR_SUPPORT = False
    chromadb = None
    SentenceTransformer = None
    np = None

logger = logging.getLogger(__name__)


class LocalStorageManager:
    """
    Free local storage solution for development.
    No cloud dependencies, works entirely offline.
    """

    def __init__(self, storage_path: str = "./crawled_data"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)

        # Create subdirectories
        self.content_dir = self.storage_path / "content"
        self.metadata_dir = self.storage_path / "metadata"
        self.vector_dir = self.storage_path / "vectors"

        for dir_path in [self.content_dir, self.metadata_dir, self.vector_dir]:
            dir_path.mkdir(exist_ok=True)

        # Initialize SQLite for metadata
        self.db_path = self.storage_path / "metadata.db"
        self.init_database()

        # Initialize vector database if available
        self.vector_client = None
        self.embedding_model = None
        if VECTOR_SUPPORT:
            self.init_vector_db()

    def init_database(self):
        """Initialize SQLite database for metadata."""
        with sqlite3.connect(self.db_path) as conn:
            # Create tables one by one
            conn.execute("""
                CREATE TABLE IF NOT EXISTS crawled_pages (
                    id TEXT PRIMARY KEY,
                    website_id TEXT NOT NULL,
                    url TEXT NOT NULL,
                    title TEXT,
                    content_hash TEXT,
                    word_count INTEGER,
                    file_path TEXT,
                    has_embeddings BOOLEAN DEFAULT FALSE,
                    crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            conn.execute("""
                CREATE TABLE IF NOT EXISTS crawl_jobs (
                    id TEXT PRIMARY KEY,
                    website_id TEXT NOT NULL,
                    status TEXT DEFAULT 'pending',
                    pages_found INTEGER DEFAULT 0,
                    pages_processed INTEGER DEFAULT 0,
                    started_at TIMESTAMP,
                    completed_at TIMESTAMP,
                    error_message TEXT,
                    config TEXT
                )
            """)

            # Create indexes
            conn.execute("CREATE INDEX IF NOT EXISTS idx_website_pages ON crawled_pages(website_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_content_hash ON crawled_pages(content_hash)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_crawl_jobs_website ON crawl_jobs(website_id)")

            conn.commit()

    def init_vector_db(self):
        """Initialize ChromaDB for vector storage."""
        try:
            self.vector_client = chromadb.PersistentClient(path=str(self.vector_dir))

            # Create collection for content embeddings
            try:
                self.collection = self.vector_client.get_collection("crawled_content")
            except:
                self.collection = self.vector_client.create_collection(
                    name="crawled_content",
                    metadata={"description": "Crawled website content for semantic search"}
                )

            # Load embedding model (free, runs locally)
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # 22MB model
            logger.info("âœ… Vector database initialized with local embeddings")

        except Exception as e:
            logger.warning(f"Vector database not available: {e}")
            self.vector_client = None

    async def store_page_content(
        self,
        page_id: str,
        website_id: str,
        url: str,
        title: str,
        content: str,
        html_content: str = None
    ) -> Dict[str, Any]:
        """Store page content locally with compression."""

        # Generate content hash
        content_hash = hashlib.sha256(content.encode()).hexdigest()

        # Create file path
        date_path = datetime.now().strftime("%Y/%m/%d")
        file_dir = self.content_dir / website_id / date_path
        file_dir.mkdir(parents=True, exist_ok=True)

        # Store compressed content
        content_file = file_dir / f"{content_hash}.txt.gz"
        html_file = file_dir / f"{content_hash}.html.gz" if html_content else None

        async with aiofiles.open(content_file, 'wb') as f:
            compressed_content = gzip.compress(content.encode())
            await f.write(compressed_content)

        if html_content:
            async with aiofiles.open(html_file, 'wb') as f:
                compressed_html = gzip.compress(html_content.encode())
                await f.write(compressed_html)

        # Store metadata in SQLite
        word_count = len(content.split())
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO crawled_pages
                (id, website_id, url, title, content_hash, word_count, file_path, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (page_id, website_id, url, title, content_hash, word_count, str(content_file)))

        # Store embeddings if vector support is available
        embedding_id = None
        if self.vector_client and self.embedding_model:
            try:
                embedding_id = await self.store_embeddings(page_id, content, {
                    'website_id': website_id,
                    'url': url,
                    'title': title,
                    'content_hash': content_hash
                })
            except Exception as e:
                logger.warning(f"Failed to store embeddings for {page_id}: {e}")

        return {
            'page_id': page_id,
            'content_hash': content_hash,
            'file_path': str(content_file),
            'word_count': word_count,
            'embedding_id': embedding_id,
            'has_vector_support': VECTOR_SUPPORT
        }

    async def store_embeddings(self, page_id: str, content: str, metadata: Dict) -> str:
        """Store content embeddings for semantic search."""
        if not self.vector_client or not self.embedding_model:
            return None

        try:
            # Generate embeddings
            embedding = self.embedding_model.encode(content)

            # Store in ChromaDB
            self.collection.add(
                embeddings=[embedding.tolist()],
                documents=[content],
                metadatas=[metadata],
                ids=[page_id]
            )

            # Update SQLite to mark as having embeddings
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    UPDATE crawled_pages SET has_embeddings = TRUE WHERE id = ?
                """, (page_id,))

            return page_id

        except Exception as e:
            logger.error(f"Failed to store embeddings: {e}")
            return None

    async def search_content(self, query: str, website_id: str = None, limit: int = 5) -> List[Dict]:
        """Semantic search through crawled content."""
        if not self.vector_client or not self.embedding_model:
            logger.warning("Vector search not available, falling back to text search")
            return await self.text_search(query, website_id, limit)

        try:
            # Generate query embedding
            query_embedding = self.embedding_model.encode(query)

            # Prepare filter
            where_clause = {"website_id": website_id} if website_id else None

            # Search similar content
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=limit,
                where=where_clause
            )

            # Format results
            search_results = []
            for i, doc_id in enumerate(results['ids'][0]):
                search_results.append({
                    'page_id': doc_id,
                    'content': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'similarity_score': 1 - results['distances'][0][i]  # Convert distance to similarity
                })

            return search_results

        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []

    async def text_search(self, query: str, website_id: str = None, limit: int = 5) -> List[Dict]:
        """Enhanced text search using SQLite FTS and TF-IDF scoring."""
        try:
            # First try FTS if available
            return await self.fts_search(query, website_id, limit)
        except:
            # Fallback to improved text search with better scoring
            return await self.improved_text_search(query, website_id, limit)

    async def fts_search(self, query: str, website_id: str = None, limit: int = 5) -> List[Dict]:
        """Full-text search using SQLite FTS5."""
        with sqlite3.connect(self.db_path) as conn:
            # Create FTS table if it doesn't exist
            try:
                conn.execute("""
                    CREATE VIRTUAL TABLE IF NOT EXISTS pages_fts USING fts5(
                        page_id, website_id, url, title, content,
                        content='crawled_pages_fts', content_rowid='rowid'
                    )
                """)

                # Check if FTS table has data
                count_result = conn.execute("SELECT COUNT(*) FROM pages_fts").fetchone()
                if count_result[0] == 0:
                    # Populate FTS table
                    await self.populate_fts_table()

            except sqlite3.OperationalError:
                # FTS not available, fall back
                raise Exception("FTS not available")

            # Prepare search query - escape special FTS characters
            fts_query = query.replace('"', '""').replace("'", "''")

            where_clause = "AND website_id = ?" if website_id else ""
            params = [website_id] if website_id else []

            cursor = conn.execute(f"""
                SELECT page_id, website_id, url, title, content, rank
                FROM pages_fts
                WHERE pages_fts MATCH ? {where_clause}
                ORDER BY rank
                LIMIT ?
            """, [fts_query] + params + [limit])

            results = []
            for row in cursor.fetchall():
                page_id, ws_id, url, title, content, rank = row
                results.append({
                    'page_id': page_id,
                    'website_id': ws_id,
                    'url': url,
                    'title': title,
                    'content': content[:500] + "..." if len(content) > 500 else content,
                    'similarity_score': min(1.0, abs(rank) / 10.0)  # Convert rank to similarity
                })

            return results

    async def improved_text_search(self, query: str, website_id: str = None, limit: int = 5) -> List[Dict]:
        """Improved text search with TF-IDF-like scoring."""
        with sqlite3.connect(self.db_path) as conn:
            where_clause = "WHERE website_id = ?" if website_id else ""
            params = [website_id] if website_id else []

            cursor = conn.execute(f"""
                SELECT id, website_id, url, title, file_path
                FROM crawled_pages
                {where_clause}
                ORDER BY updated_at DESC
            """, params)

            results = []
            query_terms = query.lower().split()

            for row in cursor.fetchall():
                page_id, website_id, url, title, file_path = row

                try:
                    content = await self.load_page_content(file_path)
                    content_lower = content.lower()
                    title_lower = title.lower()

                    # Calculate relevance score
                    score = 0.0

                    # Title matches (higher weight)
                    for term in query_terms:
                        if term in title_lower:
                            score += 2.0
                        if term in content_lower:
                            score += content_lower.count(term) * 0.1

                    # URL relevance
                    for term in query_terms:
                        if term in url.lower():
                            score += 1.0

                    if score > 0:
                        results.append({
                            'page_id': page_id,
                            'website_id': website_id,
                            'url': url,
                            'title': title,
                            'content': content[:500] + "..." if len(content) > 500 else content,
                            'similarity_score': min(1.0, score / 10.0)
                        })
                except:
                    continue

            # Sort by relevance score
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            return results[:limit]

    async def populate_fts_table(self):
        """Populate FTS table with existing content."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("SELECT id, website_id, url, title, file_path FROM crawled_pages")

            for row in cursor.fetchall():
                page_id, website_id, url, title, file_path = row
                try:
                    content = await self.load_page_content(file_path)
                    conn.execute("""
                        INSERT INTO pages_fts (page_id, website_id, url, title, content)
                        VALUES (?, ?, ?, ?, ?)
                    """, (page_id, website_id, url, title, content))
                except:
                    continue

            conn.commit()

    async def load_page_content(self, file_path: str) -> str:
        """Load compressed page content."""
        try:
            async with aiofiles.open(file_path, 'rb') as f:
                compressed_data = await f.read()
                return gzip.decompress(compressed_data).decode()
        except Exception as e:
            logger.error(f"Failed to load content from {file_path}: {e}")
            return ""

    def get_storage_stats(self) -> Dict[str, Any]:
        """Get storage usage statistics."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT
                    COUNT(*) as total_pages,
                    COUNT(DISTINCT website_id) as websites,
                    SUM(word_count) as total_words,
                    COUNT(CASE WHEN has_embeddings THEN 1 END) as pages_with_embeddings
                FROM crawled_pages
            """)
            stats = dict(zip([col[0] for col in cursor.description], cursor.fetchone()))

        # Add file system stats
        total_size = sum(f.stat().st_size for f in self.storage_path.rglob('*') if f.is_file())
        stats['storage_size_mb'] = round(total_size / (1024 * 1024), 2)
        stats['vector_support'] = VECTOR_SUPPORT

        return stats


# Usage example and testing functions
async def test_local_storage():
    """Test the local storage system."""
    storage = LocalStorageManager("./test_crawled_data")

    # Test storing content
    result = await storage.store_page_content(
        page_id="test-page-1",
        website_id="test-website",
        url="https://example.com/page1",
        title="Test Page",
        content="This is a test page with some content about artificial intelligence and machine learning.",
        html_content="<html><body><h1>Test Page</h1><p>Content here</p></body></html>"
    )

    print("âœ… Stored page:", result)

    # Test search
    if VECTOR_SUPPORT:
        search_results = await storage.search_content("artificial intelligence", "test-website")
        print("âœ… Search results:", search_results)

    # Get stats
    stats = storage.get_storage_stats()
    print("âœ… Storage stats:", stats)


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_local_storage())