"""
Crawling success and failure analytics service.
Task 5.2: Develop crawling success and failure analytics
"""

import asyncio
import time
from datetime import datetime, timezone, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
from collections import defaultdict, Counter
import statistics
import json


class FailureCategory(Enum):
    """Categories of crawling failures."""
    NETWORK_ERROR = "network_error"
    TIMEOUT = "timeout"
    JAVASCRIPT_ERROR = "javascript_error"
    AUTHENTICATION_ERROR = "authentication_error"
    CONTENT_ERROR = "content_error"
    RATE_LIMIT = "rate_limit"
    SERVER_ERROR = "server_error"
    CLIENT_ERROR = "client_error"
    RESOURCE_ERROR = "resource_error"
    PARSING_ERROR = "parsing_error"
    UNKNOWN = "unknown"


class TrendDirection(Enum):
    """Direction of trend analysis."""
    IMPROVING = "improving"
    DECLINING = "declining"
    STABLE = "stable"
    VOLATILE = "volatile"


class AlertSeverity(Enum):
    """Severity levels for analytics alerts."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class CrawlAttempt:
    """Record of a single crawl attempt."""
    url: str
    timestamp: datetime
    success: bool
    response_time: float
    status_code: Optional[int] = None
    error_message: Optional[str] = None
    retry_count: int = 0
    session_id: Optional[str] = None
    user_agent: Optional[str] = None
    content_size: Optional[int] = None
    final_url: Optional[str] = None


@dataclass
class SuccessRateMetrics:
    """Success rate metrics for different time periods."""
    overall_rate: float
    hourly_rate: float
    daily_rate: float
    weekly_rate: float
    monthly_rate: float
    total_attempts: int
    successful_attempts: int
    failed_attempts: int
    time_period: str
    calculated_at: datetime


@dataclass
class FailureAnalysis:
    """Analysis of failure patterns and categories."""
    failure_categories: Dict[str, int]
    top_failing_sites: List[Tuple[str, int]]
    failure_trends: Dict[str, List[float]]
    common_error_messages: List[Tuple[str, int]]
    failure_rate_by_hour: Dict[int, float]
    recovery_success_rate: float
    retry_effectiveness: Dict[int, float]


@dataclass
class RetryAnalysis:
    """Analysis of retry logic effectiveness."""
    retry_success_rates: Dict[int, float]  # retry_count -> success_rate
    optimal_retry_count: int
    retry_cost_effectiveness: Dict[int, float]
    recovery_strategies_effectiveness: Dict[str, float]
    retry_delay_analysis: Dict[float, float]  # delay -> success_rate


@dataclass
class TrendAnalysis:
    """Trend analysis and forecasting data."""
    success_rate_trend: List[Tuple[datetime, float]]
    failure_rate_trend: List[Tuple[datetime, float]]
    response_time_trend: List[Tuple[datetime, float]]
    trend_direction: TrendDirection
    forecast_accuracy: float
    seasonal_patterns: Dict[str, Any]
    anomalies: List[Dict[str, Any]]


@dataclass
class AnalyticsAlert:
    """Alert generated by analytics system."""
    alert_id: str
    severity: AlertSeverity
    title: str
    message: str
    metric_type: str
    current_value: float
    threshold_value: float
    url_pattern: Optional[str] = None
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    resolved: bool = False


class CrawlingAnalytics:
    """
    Comprehensive analytics service for crawling success and failure analysis.

    Features:
    - Success rate calculation and trending
    - Failure categorization and pattern analysis
    - Retry logic effectiveness monitoring
    - Performance analytics and forecasting
    - Alert generation for anomalies
    """

    def __init__(self,
                 retention_days: int = 30,
                 alert_thresholds: Optional[Dict[str, float]] = None):
        """Initialize crawling analytics service."""
        self.retention_days = retention_days
        self.crawl_attempts: List[CrawlAttempt] = []
        self.alert_thresholds = alert_thresholds or {
            'success_rate_minimum': 85.0,
            'failure_rate_maximum': 15.0,
            'response_time_maximum': 10.0,
            'retry_rate_maximum': 30.0
        }
        self.active_alerts: List[AnalyticsAlert] = []
        self._analytics_cache: Dict[str, Any] = {}
        self._cache_ttl = 300  # 5 minutes
        self._cache_timestamps: Dict[str, datetime] = {}

    async def initialize(self):
        """Initialize the analytics service."""
        await self._cleanup_old_data()

    def record_crawl_attempt(self, attempt: CrawlAttempt):
        """Record a crawl attempt for analytics."""
        self.crawl_attempts.append(attempt)

        # Trigger real-time analytics checks
        self._check_real_time_alerts(attempt)

        # Clear relevant caches
        self._invalidate_cache(['success_rates', 'failure_analysis'])

    def calculate_success_rates(self,
                              time_window: Optional[timedelta] = None,
                              url_pattern: Optional[str] = None) -> SuccessRateMetrics:
        """Calculate success rates for different time periods."""
        cache_key = f"success_rates_{time_window}_{url_pattern}"
        cached_result = self._get_cached_result(cache_key)
        if cached_result:
            return cached_result

        now = datetime.now(timezone.utc)

        # Filter attempts based on criteria
        filtered_attempts = self._filter_attempts(time_window, url_pattern)

        if not filtered_attempts:
            return SuccessRateMetrics(
                overall_rate=0.0, hourly_rate=0.0, daily_rate=0.0,
                weekly_rate=0.0, monthly_rate=0.0, total_attempts=0,
                successful_attempts=0, failed_attempts=0,
                time_period=str(time_window) if time_window else "all_time",
                calculated_at=now
            )

        total_attempts = len(filtered_attempts)
        successful_attempts = sum(1 for attempt in filtered_attempts if attempt.success)
        failed_attempts = total_attempts - successful_attempts

        overall_rate = (successful_attempts / total_attempts) * 100

        # Calculate time-based rates
        hourly_rate = self._calculate_time_based_rate(filtered_attempts, timedelta(hours=1))
        daily_rate = self._calculate_time_based_rate(filtered_attempts, timedelta(days=1))
        weekly_rate = self._calculate_time_based_rate(filtered_attempts, timedelta(weeks=1))
        monthly_rate = self._calculate_time_based_rate(filtered_attempts, timedelta(days=30))

        result = SuccessRateMetrics(
            overall_rate=overall_rate,
            hourly_rate=hourly_rate,
            daily_rate=daily_rate,
            weekly_rate=weekly_rate,
            monthly_rate=monthly_rate,
            total_attempts=total_attempts,
            successful_attempts=successful_attempts,
            failed_attempts=failed_attempts,
            time_period=str(time_window) if time_window else "all_time",
            calculated_at=now
        )

        self._cache_result(cache_key, result)
        return result

    def analyze_failures(self,
                        time_window: Optional[timedelta] = None) -> FailureAnalysis:
        """Analyze failure patterns and categorize failures."""
        cache_key = f"failure_analysis_{time_window}"
        cached_result = self._get_cached_result(cache_key)
        if cached_result:
            return cached_result

        filtered_attempts = self._filter_attempts(time_window)
        failed_attempts = [a for a in filtered_attempts if not a.success]

        if not failed_attempts:
            return FailureAnalysis(
                failure_categories={}, top_failing_sites=[],
                failure_trends={}, common_error_messages=[],
                failure_rate_by_hour={}, recovery_success_rate=0.0,
                retry_effectiveness={}
            )

        # Categorize failures
        failure_categories = self._categorize_failures(failed_attempts)

        # Find top failing sites
        site_failures = Counter(attempt.url for attempt in failed_attempts)
        top_failing_sites = site_failures.most_common(10)

        # Analyze failure trends over time
        failure_trends = self._analyze_failure_trends(failed_attempts)

        # Common error messages
        error_messages = [a.error_message for a in failed_attempts if a.error_message]
        common_errors = Counter(error_messages).most_common(10)

        # Failure rate by hour of day
        failure_rate_by_hour = self._calculate_hourly_failure_rates(failed_attempts)

        # Recovery success rate (retries that eventually succeeded)
        recovery_success_rate = self._calculate_recovery_success_rate()

        # Retry effectiveness
        retry_effectiveness = self._analyze_retry_effectiveness()

        result = FailureAnalysis(
            failure_categories=failure_categories,
            top_failing_sites=top_failing_sites,
            failure_trends=failure_trends,
            common_error_messages=common_errors,
            failure_rate_by_hour=failure_rate_by_hour,
            recovery_success_rate=recovery_success_rate,
            retry_effectiveness=retry_effectiveness
        )

        self._cache_result(cache_key, result)
        return result

    def analyze_retry_effectiveness(self) -> RetryAnalysis:
        """Analyze the effectiveness of retry logic."""
        cache_key = "retry_analysis"
        cached_result = self._get_cached_result(cache_key)
        if cached_result:
            return cached_result

        retry_attempts = [a for a in self.crawl_attempts if a.retry_count > 0]

        if not retry_attempts:
            return RetryAnalysis(
                retry_success_rates={}, optimal_retry_count=0,
                retry_cost_effectiveness={}, recovery_strategies_effectiveness={},
                retry_delay_analysis={}
            )

        # Calculate success rates by retry count
        retry_success_rates = {}
        for retry_count in range(1, 6):  # Analyze up to 5 retries
            attempts_at_count = [a for a in retry_attempts if a.retry_count == retry_count]
            if attempts_at_count:
                success_rate = sum(1 for a in attempts_at_count if a.success) / len(attempts_at_count)
                retry_success_rates[retry_count] = success_rate * 100

        # Find optimal retry count (best success rate vs cost)
        optimal_retry_count = self._find_optimal_retry_count(retry_success_rates)

        # Cost effectiveness (success rate / retry cost)
        retry_cost_effectiveness = {}
        for count, success_rate in retry_success_rates.items():
            cost = count * 1.0  # Simple cost model
            retry_cost_effectiveness[count] = success_rate / cost if cost > 0 else 0

        # Recovery strategies effectiveness (mock data for now)
        recovery_strategies_effectiveness = {
            'increase_timeout': 75.0,
            'disable_javascript': 60.0,
            'change_user_agent': 45.0,
            'use_proxy': 80.0
        }

        # Retry delay analysis (mock data for now)
        retry_delay_analysis = {
            1.0: 65.0,  # 1 second delay
            2.0: 72.0,  # 2 second delay
            5.0: 78.0,  # 5 second delay
            10.0: 75.0  # 10 second delay
        }

        result = RetryAnalysis(
            retry_success_rates=retry_success_rates,
            optimal_retry_count=optimal_retry_count,
            retry_cost_effectiveness=retry_cost_effectiveness,
            recovery_strategies_effectiveness=recovery_strategies_effectiveness,
            retry_delay_analysis=retry_delay_analysis
        )

        self._cache_result(cache_key, result)
        return result

    def analyze_trends(self,
                      days_back: int = 7) -> TrendAnalysis:
        """Analyze trends and patterns in crawling data."""
        cache_key = f"trend_analysis_{days_back}"
        cached_result = self._get_cached_result(cache_key)
        if cached_result:
            return cached_result

        cutoff_time = datetime.now(timezone.utc) - timedelta(days=days_back)
        recent_attempts = [a for a in self.crawl_attempts if a.timestamp >= cutoff_time]

        if len(recent_attempts) < 10:  # Need minimum data for trends
            return TrendAnalysis(
                success_rate_trend=[], failure_rate_trend=[],
                response_time_trend=[], trend_direction=TrendDirection.STABLE,
                forecast_accuracy=0.0, seasonal_patterns={},
                anomalies=[]
            )

        # Calculate daily trends
        daily_trends = self._calculate_daily_trends(recent_attempts)

        success_rate_trend = daily_trends['success_rate']
        failure_rate_trend = daily_trends['failure_rate']
        response_time_trend = daily_trends['response_time']

        # Determine overall trend direction
        trend_direction = self._determine_trend_direction(success_rate_trend)

        # Calculate forecast accuracy (simplified)
        forecast_accuracy = self._calculate_forecast_accuracy(success_rate_trend)

        # Identify seasonal patterns
        seasonal_patterns = self._identify_seasonal_patterns(recent_attempts)

        # Detect anomalies
        anomalies = self._detect_anomalies(recent_attempts)

        result = TrendAnalysis(
            success_rate_trend=success_rate_trend,
            failure_rate_trend=failure_rate_trend,
            response_time_trend=response_time_trend,
            trend_direction=trend_direction,
            forecast_accuracy=forecast_accuracy,
            seasonal_patterns=seasonal_patterns,
            anomalies=anomalies
        )

        self._cache_result(cache_key, result)
        return result

    def get_analytics_summary(self) -> Dict[str, Any]:
        """Get comprehensive analytics summary."""
        success_rates = self.calculate_success_rates()
        failure_analysis = self.analyze_failures()
        retry_analysis = self.analyze_retry_effectiveness()
        trend_analysis = self.analyze_trends()

        return {
            'success_rates': {
                'overall': success_rates.overall_rate,
                'daily': success_rates.daily_rate,
                'total_attempts': success_rates.total_attempts
            },
            'failure_analysis': {
                'categories': failure_analysis.failure_categories,
                'top_failing_sites': failure_analysis.top_failing_sites[:5],
                'recovery_rate': failure_analysis.recovery_success_rate
            },
            'retry_effectiveness': {
                'optimal_retry_count': retry_analysis.optimal_retry_count,
                'success_rates': retry_analysis.retry_success_rates
            },
            'trends': {
                'direction': trend_analysis.trend_direction.value,
                'anomalies_count': len(trend_analysis.anomalies)
            },
            'alerts': {
                'active_count': len(self.active_alerts),
                'critical_count': len([a for a in self.active_alerts if a.severity == AlertSeverity.CRITICAL])
            }
        }

    def get_site_specific_analytics(self, url_pattern: str) -> Dict[str, Any]:
        """Get analytics specific to a site or URL pattern."""
        attempts = [a for a in self.crawl_attempts if url_pattern in a.url]

        if not attempts:
            return {'error': 'No data found for the specified URL pattern'}

        total_attempts = len(attempts)
        successful = sum(1 for a in attempts if a.success)
        success_rate = (successful / total_attempts) * 100

        avg_response_time = statistics.mean(a.response_time for a in attempts)

        # Recent performance (last 24 hours)
        recent_cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
        recent_attempts = [a for a in attempts if a.timestamp >= recent_cutoff]
        recent_success_rate = 0.0
        if recent_attempts:
            recent_successful = sum(1 for a in recent_attempts if a.success)
            recent_success_rate = (recent_successful / len(recent_attempts)) * 100

        return {
            'url_pattern': url_pattern,
            'total_attempts': total_attempts,
            'success_rate': success_rate,
            'recent_success_rate': recent_success_rate,
            'average_response_time': avg_response_time,
            'last_crawl': max(a.timestamp for a in attempts) if attempts else None
        }

    def get_active_alerts(self) -> List[AnalyticsAlert]:
        """Get all active alerts."""
        return [alert for alert in self.active_alerts if not alert.resolved]

    def resolve_alert(self, alert_id: str) -> bool:
        """Resolve an active alert."""
        for alert in self.active_alerts:
            if alert.alert_id == alert_id:
                alert.resolved = True
                return True
        return False

    async def generate_analytics_report(self,
                                      format_type: str = "json") -> str:
        """Generate comprehensive analytics report."""
        summary = self.get_analytics_summary()
        success_rates = self.calculate_success_rates()
        failure_analysis = self.analyze_failures()
        retry_analysis = self.analyze_retry_effectiveness()
        trend_analysis = self.analyze_trends()

        report_data = {
            'generated_at': datetime.now(timezone.utc).isoformat(),
            'summary': summary,
            'detailed_metrics': {
                'success_rates': success_rates.__dict__,
                'failure_analysis': {
                    'categories': failure_analysis.failure_categories,
                    'top_failing_sites': failure_analysis.top_failing_sites,
                    'recovery_rate': failure_analysis.recovery_success_rate
                },
                'retry_analysis': retry_analysis.__dict__,
                'trends': {
                    'direction': trend_analysis.trend_direction.value,
                    'forecast_accuracy': trend_analysis.forecast_accuracy,
                    'anomalies_count': len(trend_analysis.anomalies)
                }
            },
            'active_alerts': [alert.__dict__ for alert in self.get_active_alerts()]
        }

        if format_type == "json":
            return json.dumps(report_data, indent=2, default=str)
        else:
            # Could implement other formats (CSV, HTML, etc.)
            return json.dumps(report_data, indent=2, default=str)

    # Private helper methods

    def _filter_attempts(self,
                        time_window: Optional[timedelta] = None,
                        url_pattern: Optional[str] = None) -> List[CrawlAttempt]:
        """Filter crawl attempts based on criteria."""
        attempts = self.crawl_attempts.copy()

        if time_window:
            cutoff_time = datetime.now(timezone.utc) - time_window
            attempts = [a for a in attempts if a.timestamp >= cutoff_time]

        if url_pattern:
            attempts = [a for a in attempts if url_pattern in a.url]

        return attempts

    def _calculate_time_based_rate(self, attempts: List[CrawlAttempt],
                                  time_window: timedelta) -> float:
        """Calculate success rate for a specific time window."""
        cutoff_time = datetime.now(timezone.utc) - time_window
        recent_attempts = [a for a in attempts if a.timestamp >= cutoff_time]

        if not recent_attempts:
            return 0.0

        successful = sum(1 for a in recent_attempts if a.success)
        return (successful / len(recent_attempts)) * 100

    def _categorize_failures(self, failed_attempts: List[CrawlAttempt]) -> Dict[str, int]:
        """Categorize failures by type."""
        categories = defaultdict(int)

        for attempt in failed_attempts:
            category = self._classify_failure(attempt)
            categories[category.value] += 1

        return dict(categories)

    def _classify_failure(self, attempt: CrawlAttempt) -> FailureCategory:
        """Classify a single failure into a category."""
        if not attempt.error_message:
            return FailureCategory.UNKNOWN

        error_msg = attempt.error_message.lower()

        if 'timeout' in error_msg or 'time out' in error_msg:
            return FailureCategory.TIMEOUT
        elif 'network' in error_msg or 'connection' in error_msg:
            return FailureCategory.NETWORK_ERROR
        elif 'javascript' in error_msg or 'js' in error_msg:
            return FailureCategory.JAVASCRIPT_ERROR
        elif 'auth' in error_msg or 'unauthorized' in error_msg:
            return FailureCategory.AUTHENTICATION_ERROR
        elif 'rate limit' in error_msg or 'too many requests' in error_msg:
            return FailureCategory.RATE_LIMIT
        elif attempt.status_code and 500 <= attempt.status_code < 600:
            return FailureCategory.SERVER_ERROR
        elif attempt.status_code and 400 <= attempt.status_code < 500:
            return FailureCategory.CLIENT_ERROR
        elif 'memory' in error_msg or 'resource' in error_msg:
            return FailureCategory.RESOURCE_ERROR
        elif 'parse' in error_msg or 'parsing' in error_msg:
            return FailureCategory.PARSING_ERROR
        else:
            return FailureCategory.UNKNOWN

    def _analyze_failure_trends(self, failed_attempts: List[CrawlAttempt]) -> Dict[str, List[float]]:
        """Analyze failure trends over time."""
        # Group by day and calculate failure rates
        daily_failures = defaultdict(list)

        for attempt in failed_attempts:
            day_key = attempt.timestamp.date().isoformat()
            daily_failures[day_key].append(attempt)

        trends = {}
        for category in FailureCategory:
            category_trends = []
            for day, day_failures in daily_failures.items():
                category_count = sum(1 for f in day_failures
                                   if self._classify_failure(f) == category)
                category_rate = (category_count / len(day_failures)) * 100
                category_trends.append(category_rate)
            trends[category.value] = category_trends

        return trends

    def _calculate_hourly_failure_rates(self, failed_attempts: List[CrawlAttempt]) -> Dict[int, float]:
        """Calculate failure rates by hour of day."""
        hourly_failures = defaultdict(int)
        hourly_totals = defaultdict(int)

        # Count all attempts by hour
        for attempt in self.crawl_attempts:
            hour = attempt.timestamp.hour
            hourly_totals[hour] += 1
            if not attempt.success:
                hourly_failures[hour] += 1

        hourly_rates = {}
        for hour in range(24):
            if hourly_totals[hour] > 0:
                rate = (hourly_failures[hour] / hourly_totals[hour]) * 100
                hourly_rates[hour] = rate
            else:
                hourly_rates[hour] = 0.0

        return hourly_rates

    def _calculate_recovery_success_rate(self) -> float:
        """Calculate success rate of recovery attempts."""
        retry_attempts = [a for a in self.crawl_attempts if a.retry_count > 0]
        if not retry_attempts:
            return 0.0

        successful_recoveries = sum(1 for a in retry_attempts if a.success)
        return (successful_recoveries / len(retry_attempts)) * 100

    def _analyze_retry_effectiveness(self) -> Dict[int, float]:
        """Analyze effectiveness of different retry counts."""
        retry_effectiveness = {}

        for retry_count in range(1, 6):
            attempts_at_count = [a for a in self.crawl_attempts
                               if a.retry_count == retry_count]
            if attempts_at_count:
                success_rate = sum(1 for a in attempts_at_count if a.success) / len(attempts_at_count)
                retry_effectiveness[retry_count] = success_rate * 100

        return retry_effectiveness

    def _find_optimal_retry_count(self, retry_success_rates: Dict[int, float]) -> int:
        """Find optimal retry count based on success rate vs cost."""
        if not retry_success_rates:
            return 0

        best_score = 0
        optimal_count = 0

        for count, success_rate in retry_success_rates.items():
            # Simple scoring: success_rate / cost (where cost = retry_count)
            score = success_rate / count if count > 0 else 0
            if score > best_score:
                best_score = score
                optimal_count = count

        return optimal_count

    def _calculate_daily_trends(self, attempts: List[CrawlAttempt]) -> Dict[str, List[Tuple[datetime, float]]]:
        """Calculate daily trends for various metrics."""
        daily_data = defaultdict(list)

        # Group by day
        for attempt in attempts:
            day_key = attempt.timestamp.date()
            daily_data[day_key].append(attempt)

        trends = {
            'success_rate': [],
            'failure_rate': [],
            'response_time': []
        }

        for day, day_attempts in sorted(daily_data.items()):
            day_datetime = datetime.combine(day, datetime.min.time()).replace(tzinfo=timezone.utc)

            # Success rate
            successful = sum(1 for a in day_attempts if a.success)
            success_rate = (successful / len(day_attempts)) * 100
            trends['success_rate'].append((day_datetime, success_rate))

            # Failure rate
            failure_rate = 100 - success_rate
            trends['failure_rate'].append((day_datetime, failure_rate))

            # Average response time
            avg_response_time = statistics.mean(a.response_time for a in day_attempts)
            trends['response_time'].append((day_datetime, avg_response_time))

        return trends

    def _determine_trend_direction(self, success_rate_trend: List[Tuple[datetime, float]]) -> TrendDirection:
        """Determine the overall trend direction."""
        if len(success_rate_trend) < 3:
            return TrendDirection.STABLE

        values = [rate for _, rate in success_rate_trend]

        # Calculate moving average slope
        if len(values) >= 3:
            recent_avg = statistics.mean(values[-3:])
            earlier_avg = statistics.mean(values[:3])

            diff = recent_avg - earlier_avg

            if diff > 5:
                return TrendDirection.IMPROVING
            elif diff < -5:
                return TrendDirection.DECLINING
            else:
                # Check volatility
                variance = statistics.variance(values) if len(values) > 1 else 0
                if variance > 100:  # High variance
                    return TrendDirection.VOLATILE
                else:
                    return TrendDirection.STABLE

        return TrendDirection.STABLE

    def _calculate_forecast_accuracy(self, trend_data: List[Tuple[datetime, float]]) -> float:
        """Calculate forecast accuracy (simplified)."""
        if len(trend_data) < 5:
            return 0.0

        # Simple accuracy calculation based on trend consistency
        values = [rate for _, rate in trend_data]
        variance = statistics.variance(values) if len(values) > 1 else 0

        # Higher variance = lower accuracy
        max_variance = 100
        accuracy = max(0, 100 - (variance / max_variance) * 100)
        return min(100, accuracy)

    def _identify_seasonal_patterns(self, attempts: List[CrawlAttempt]) -> Dict[str, Any]:
        """Identify seasonal patterns in crawling data."""
        hourly_patterns = defaultdict(list)
        daily_patterns = defaultdict(list)

        for attempt in attempts:
            hour = attempt.timestamp.hour
            day_of_week = attempt.timestamp.weekday()

            hourly_patterns[hour].append(1 if attempt.success else 0)
            daily_patterns[day_of_week].append(1 if attempt.success else 0)

        # Calculate average success rates
        hourly_avg = {}
        for hour, successes in hourly_patterns.items():
            hourly_avg[hour] = statistics.mean(successes) * 100

        daily_avg = {}
        for day, successes in daily_patterns.items():
            daily_avg[day] = statistics.mean(successes) * 100

        return {
            'hourly_patterns': hourly_avg,
            'daily_patterns': daily_avg,
            'peak_performance_hour': max(hourly_avg, key=hourly_avg.get) if hourly_avg else None,
            'best_day_of_week': max(daily_avg, key=daily_avg.get) if daily_avg else None
        }

    def _detect_anomalies(self, attempts: List[CrawlAttempt]) -> List[Dict[str, Any]]:
        """Detect anomalies in crawling data."""
        anomalies = []

        if len(attempts) < 10:
            return anomalies

        # Group by hour for anomaly detection
        hourly_data = defaultdict(list)
        for attempt in attempts:
            hour_key = attempt.timestamp.replace(minute=0, second=0, microsecond=0)
            hourly_data[hour_key].append(attempt)

        # Calculate baseline metrics
        all_hourly_success_rates = []
        all_hourly_response_times = []

        for hour_attempts in hourly_data.values():
            if hour_attempts:
                success_rate = sum(1 for a in hour_attempts if a.success) / len(hour_attempts)
                avg_response_time = statistics.mean(a.response_time for a in hour_attempts)
                all_hourly_success_rates.append(success_rate * 100)
                all_hourly_response_times.append(avg_response_time)

        if len(all_hourly_success_rates) < 3:
            return anomalies

        # Calculate thresholds
        success_mean = statistics.mean(all_hourly_success_rates)
        success_stdev = statistics.stdev(all_hourly_success_rates) if len(all_hourly_success_rates) > 1 else 0

        response_mean = statistics.mean(all_hourly_response_times)
        response_stdev = statistics.stdev(all_hourly_response_times) if len(all_hourly_response_times) > 1 else 0

        # Detect anomalies (values outside 2 standard deviations)
        for hour, hour_attempts in hourly_data.items():
            if not hour_attempts:
                continue

            success_rate = sum(1 for a in hour_attempts if a.success) / len(hour_attempts) * 100
            avg_response_time = statistics.mean(a.response_time for a in hour_attempts)

            # Success rate anomaly
            if abs(success_rate - success_mean) > 2 * success_stdev:
                anomalies.append({
                    'type': 'success_rate_anomaly',
                    'timestamp': hour,
                    'value': success_rate,
                    'expected_range': (success_mean - 2 * success_stdev, success_mean + 2 * success_stdev),
                    'severity': 'high' if abs(success_rate - success_mean) > 3 * success_stdev else 'medium'
                })

            # Response time anomaly
            if abs(avg_response_time - response_mean) > 2 * response_stdev:
                anomalies.append({
                    'type': 'response_time_anomaly',
                    'timestamp': hour,
                    'value': avg_response_time,
                    'expected_range': (response_mean - 2 * response_stdev, response_mean + 2 * response_stdev),
                    'severity': 'high' if abs(avg_response_time - response_mean) > 3 * response_stdev else 'medium'
                })

        return anomalies

    def _check_real_time_alerts(self, attempt: CrawlAttempt):
        """Check for real-time alerts based on new attempt."""
        # Success rate alert
        recent_attempts = self._filter_attempts(timedelta(hours=1))
        if len(recent_attempts) >= 10:
            success_rate = sum(1 for a in recent_attempts if a.success) / len(recent_attempts) * 100

            if success_rate < self.alert_thresholds['success_rate_minimum']:
                alert = AnalyticsAlert(
                    alert_id=f"success_rate_{int(time.time())}",
                    severity=AlertSeverity.HIGH if success_rate < 70 else AlertSeverity.MEDIUM,
                    title="Low Success Rate Alert",
                    message=f"Success rate dropped to {success_rate:.1f}% in the last hour",
                    metric_type="success_rate",
                    current_value=success_rate,
                    threshold_value=self.alert_thresholds['success_rate_minimum']
                )
                self.active_alerts.append(alert)

        # Response time alert
        if attempt.response_time > self.alert_thresholds['response_time_maximum']:
            alert = AnalyticsAlert(
                alert_id=f"response_time_{int(time.time())}",
                severity=AlertSeverity.MEDIUM,
                title="High Response Time Alert",
                message=f"Response time of {attempt.response_time:.2f}s exceeds threshold",
                metric_type="response_time",
                current_value=attempt.response_time,
                threshold_value=self.alert_thresholds['response_time_maximum'],
                url_pattern=attempt.url
            )
            self.active_alerts.append(alert)

    def _get_cached_result(self, cache_key: str) -> Optional[Any]:
        """Get cached result if still valid."""
        if cache_key in self._analytics_cache:
            cache_time = self._cache_timestamps.get(cache_key)
            if cache_time and (datetime.now(timezone.utc) - cache_time).seconds < self._cache_ttl:
                return self._analytics_cache[cache_key]
        return None

    def _cache_result(self, cache_key: str, result: Any):
        """Cache a result with timestamp."""
        self._analytics_cache[cache_key] = result
        self._cache_timestamps[cache_key] = datetime.now(timezone.utc)

    def _invalidate_cache(self, cache_keys: List[str]):
        """Invalidate specific cache entries."""
        for key in cache_keys:
            self._analytics_cache.pop(key, None)
            self._cache_timestamps.pop(key, None)

    async def _cleanup_old_data(self):
        """Clean up old crawl attempts based on retention policy."""
        cutoff_time = datetime.now(timezone.utc) - timedelta(days=self.retention_days)
        self.crawl_attempts = [a for a in self.crawl_attempts if a.timestamp >= cutoff_time]

        # Clean up old alerts
        alert_cutoff = datetime.now(timezone.utc) - timedelta(days=7)
        self.active_alerts = [a for a in self.active_alerts if a.created_at >= alert_cutoff]


# Global analytics instance
_analytics_instance: Optional[CrawlingAnalytics] = None


async def get_crawling_analytics(retention_days: int = 30) -> CrawlingAnalytics:
    """Get or create global crawling analytics instance."""
    global _analytics_instance

    if _analytics_instance is None:
        _analytics_instance = CrawlingAnalytics(retention_days=retention_days)
        await _analytics_instance.initialize()

    return _analytics_instance


async def shutdown_crawling_analytics():
    """Shutdown global crawling analytics instance."""
    global _analytics_instance

    if _analytics_instance is not None:
        # Perform any cleanup if needed
        _analytics_instance = None